{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 738 Comment Toxicity Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "import tools\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Removing Punctuation...\n",
      "Building Stop Word Dictionary...\n",
      "About to get all good comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src\\tools.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  comments_scores['comment_text'] = self.processCommentList(comments_scores['comment_text'])\n"
     ]
    }
   ],
   "source": [
    "# establish file toolkit\n",
    "t = tools.tools('../data/train.csv')\n",
    "df = t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0.000000\n",
      "1          0.000000\n",
      "2          0.000000\n",
      "3          0.000000\n",
      "4          0.893617\n",
      "5          0.666667\n",
      "6          0.457627\n",
      "7          0.000000\n",
      "8          0.000000\n",
      "9          0.000000\n",
      "10         0.000000\n",
      "11         0.440000\n",
      "12         0.000000\n",
      "13         0.600000\n",
      "14         0.500000\n",
      "15         0.000000\n",
      "16         0.000000\n",
      "17         0.000000\n",
      "18         0.000000\n",
      "19         0.500000\n",
      "20         0.000000\n",
      "21         0.000000\n",
      "22         0.000000\n",
      "23         0.000000\n",
      "24         0.000000\n",
      "25         0.000000\n",
      "26         0.000000\n",
      "27         0.000000\n",
      "28         0.000000\n",
      "29         0.000000\n",
      "             ...   \n",
      "1804844    0.000000\n",
      "1804845    0.000000\n",
      "1804846    0.000000\n",
      "1804847    0.000000\n",
      "1804848    0.000000\n",
      "1804849    0.400000\n",
      "1804850    0.000000\n",
      "1804851    0.000000\n",
      "1804852    0.300000\n",
      "1804853    0.000000\n",
      "1804854    0.000000\n",
      "1804855    0.166667\n",
      "1804856    0.500000\n",
      "1804857    0.700000\n",
      "1804858    0.200000\n",
      "1804859    0.000000\n",
      "1804860    0.000000\n",
      "1804861    0.000000\n",
      "1804862    0.000000\n",
      "1804863    0.200000\n",
      "1804864    0.000000\n",
      "1804865    0.166667\n",
      "1804866    0.300000\n",
      "1804867    0.200000\n",
      "1804868    0.000000\n",
      "1804869    0.000000\n",
      "1804870    0.000000\n",
      "1804871    0.000000\n",
      "1804872    0.621212\n",
      "1804873    0.000000\n",
      "Name: target, Length: 1804874, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CommentPredictor as CP\n",
    "mod = CP.CommentPredictor(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep/Weight Mapping\n",
    "Here we prep the text corpus for neural network training. getProcessedComments gives us all comments, lowercased with punctuation and Unicode characters stripped. We then replace each word in each comment with the calculated preliminary toxicity weight we have calculated for that word. We pad/truncate comments so that each comment is a 200-word sequence, and create a list of target toxicity labels corresponding to these comments. These two lists (input and output) will form the training set for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Punctuation...\n",
      "Training set size: 1804874\n",
      "Unique words: 287025\n"
     ]
    }
   ],
   "source": [
    "procComms = (t.getProcessedComments())\n",
    "labels = df['target']\n",
    "wordDict = mod.word_weight_dict\n",
    "print(\"Training set size: \" + str(len(procComms)))\n",
    "for index, comment in enumerate(procComms):\n",
    "    split = text_to_word_sequence(comment)\n",
    "    for j, word in enumerate(split):\n",
    "        split[j] = wordDict[word]\n",
    "    procComms[index] = split\n",
    "uniqueWords = len(wordDict)\n",
    "commentLen = 200\n",
    "x = pad_sequences(procComms, maxlen=commentLen, value=0.0, dtype='float32', padding=\"post\", truncating=\"post\")\n",
    "y = labels\n",
    "\n",
    "\n",
    "print(\"Unique words: \" + str(uniqueWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   0.66 0.   0.   0.66 0.   0.   0.66 0.   0.66 0.   0.66\n",
      " 0.   0.66 0.66 0.66 0.66 0.66 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.  ]\n",
      "[0.0, 0.0, 0.0, 0.66, 0.0, 0.0, 0.66, 0.0, 0.0, 0.66, 0.0, 0.66, 0.0, 0.66, 0.0, 0.66, 0.66, 0.66, 0.66, 0.66]\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(procComms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Punctuation...\n"
     ]
    }
   ],
   "source": [
    "tdf = pd.read_csv('../data/test.csv')\n",
    "testProcComms = t.processCommentList(tdf['comment_text'])\n",
    "for index, comment in enumerate(testProcComms):\n",
    "    split = text_to_word_sequence(comment)\n",
    "    for j, word in enumerate(split):\n",
    "        if word in wordDict:\n",
    "            split[j] = wordDict[word]\n",
    "        else: split[j] = 0.0\n",
    "    testProcComms[index] = split\n",
    "xTest = pad_sequences(testProcComms, maxlen=commentLen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Construction\n",
    "We now initialize a very simple neural network to predict the toxicity value of each comment, represented as a sequence of 200 word-toxicity weights. These serve as the inputs to 200 input neurons. We utilize two standard, 50-neuron dense layers and two dropout layers, which each drop 1/10 of their nodes from the network at random. This encourages the network to generalize and avoid overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 12,651\n",
      "Trainable params: 12,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input layer for an arbitrary number of comments of length commentLen\n",
    "inputLayer = Input(shape=(commentLen, ))    \n",
    "#drop 1/10 nodes to improve generalization\n",
    "n = Dropout(rate=.9)(inputLayer)\n",
    "#standard nn\n",
    "n = Dense(50, activation='relu')(n)\n",
    "#drop again\n",
    "n = Dropout(rate=.9)(n)\n",
    "#standard nn, sigmoid for values between 0 and 1\n",
    "n = Dense(50, activation='relu')(n)\n",
    "n = Dense(1, activation='sigmoid')(n)\n",
    "model = Model(inputs=inputLayer, outputs=n)\n",
    "adamOpt = optimizers.Adam(lr=.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adamOpt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training\n",
    "Train the neural network on the word-weight lists for each comment, trying to predict toxicity score.\n",
    "Use SGD mini-batches of size 32 and 2 epochs. Remove 1/10 of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/1\n",
      " 910336/1624386 [===============>..............] - ETA: 54s - loss: 0.3516 - acc: 0.6987"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=32, epochs=1, validation_split=.1)\n",
    "preds = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
