{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 738 Comment Toxicity Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "import tools\n",
    "\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Removing Punctuation...\n",
      "Building Stop Word Dictionary...\n",
      "About to get all good comments\n"
     ]
    }
   ],
   "source": [
    "# establish file toolkit\n",
    "t = tools.tools('../data/train.csv')\n",
    "df = t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CommentPredictor as CP\n",
    "mod = CP.CommentPredictor(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep/Weight Mapping\n",
    "Here we prep the text corpus for neural network training. getProcessedComments gives us all comments, lowercased with punctuation and Unicode characters stripped. We then replace each word in each comment with the calculated preliminary toxicity weight we have calculated for that word. We pad/truncate comments so that each comment is a 200-word sequence, and create a list of target toxicity labels corresponding to these comments. These two lists (input and output) will form the training set for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Punctuation...\n",
      "Training set size: 1804874\n",
      "Unique words: 287025\n"
     ]
    }
   ],
   "source": [
    "procComms = (t.getProcessedComments())\n",
    "labels = df['target']\n",
    "wordDict = mod.word_weight_dict\n",
    "print(\"Training set size: \" + str(len(procComms)))\n",
    "for index, comment in enumerate(procComms):\n",
    "    split = text_to_word_sequence(comment)\n",
    "    for j, word in enumerate(split):\n",
    "        split[j] = wordDict[word]\n",
    "    procComms[index] = split\n",
    "uniqueWords = len(wordDict)\n",
    "commentLen = 200\n",
    "x = pad_sequences(procComms, maxlen=commentLen, value=0.0, dtype='float32', padding=\"post\", truncating=\"post\")\n",
    "y = labels\n",
    "\n",
    "\n",
    "print(\"Unique words: \" + str(uniqueWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  0.5 0.  0.  0.5 0.  0.  0.5 0.  0.5 0.  0.5 0.  0.5 0.5 0.5\n",
      " 0.5 0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0. ]\n",
      "[0, 0, 0, 0.5, 0, 0, 0.5, 0, 0, 0.5, 0, 0.5, 0, 0.5, 0, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(procComms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Punctuation...\n"
     ]
    }
   ],
   "source": [
    "tdf = pd.read_csv('../data/test.csv')\n",
    "testProcComms = t.processCommentList(tdf['comment_text'])\n",
    "for index, comment in enumerate(testProcComms):\n",
    "    split = text_to_word_sequence(comment)\n",
    "    for j, word in enumerate(split):\n",
    "        if word in wordDict:\n",
    "            split[j] = wordDict[word]\n",
    "        else: split[j] = 0.0\n",
    "    testProcComms[index] = split\n",
    "xTest = pad_sequences(testProcComms, maxlen=commentLen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Construction\n",
    "We now initialize a very simple neural network to predict the toxicity value of each comment, represented as a sequence of 200 word-toxicity weights. These serve as the inputs to 200 input neurons. We utilize two standard, 50-neuron dense layers and two dropout layers, which each drop 1/10 of their nodes from the network at random. This encourages the network to generalize and avoid overfitting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 12,651\n",
      "Trainable params: 12,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input layer for an arbitrary number of comments of length commentLen\n",
    "inputLayer = Input(shape=(commentLen, ))    \n",
    "#drop 1/10 nodes to improve generalization\n",
    "n = Dropout(rate=.9)(inputLayer)\n",
    "#standard nn\n",
    "n = Dense(50, activation='relu')(inputLayer)\n",
    "#drop again\n",
    "n = Dropout(rate=.9)(n)\n",
    "#standard nn, sigmoid for values between 0 and 1\n",
    "n = Dense(50, activation='relu')(n)\n",
    "n = Dense(1, activation='sigmoid')(n)\n",
    "model = Model(inputs=inputLayer, outputs=n)\n",
    "sgdOpt = optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgdOpt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training\n",
    "Train the neural network on the word-weight lists for each comment, trying to predict toxicity score.\n",
    "Use SGD mini-batches of size 32 and 2 epochs. Remove 1/10 of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1624386 samples, validate on 180488 samples\n",
      "Epoch 1/1\n",
      "1624386/1624386 [==============================] - 193s 119us/step - loss: 0.3678 - acc: 0.6981 - val_loss: 0.3461 - val_acc: 0.6879\n"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=10, epochs=2, validation_split=.1)\n",
    "preds = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "The neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\n",
      "[0.12665713]\n",
      "haha you guys are a bunch of losers.\n",
      "[0.12853405]\n"
     ]
    }
   ],
   "source": [
    "trainComments = df[\"comment_text\"]\n",
    "print(trainComments[0])\n",
    "print(preds[0])\n",
    "print(trainComments[4])\n",
    "print(preds[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8936170212765957\n"
     ]
    }
   ],
   "source": [
    "print(y[0])\n",
    "print(y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
